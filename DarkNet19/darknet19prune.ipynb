{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['darknet']\n",
    "defaultcfg = {\n",
    "    19 : [[32,3],'M',[64,3],'M',[128,3],[64,1],[128,3],'M',[256,3],[128,1],[256,3],'M',[512,3],[256,1],[512,3],[256,1],[512,3],'M',[1024,3],[512,1],[1024,3],[512,1],[1024,3]]\n",
    "}\n",
    "\n",
    "class darknet(nn.Module):\n",
    "    def __init__(self, depth=19, init_weights=False, cfg=None):\n",
    "        super(darknet, self).__init__()\n",
    "        if cfg is None:\n",
    "            cfg = defaultcfg[depth]\n",
    "\n",
    "        self.feature = self.make_layers(cfg, True)\n",
    "        num_classes = 30\n",
    "        self.classifier = nn.Sequential(nn.Conv2d(cfg[-1][0], out_channels=num_classes, kernel_size=1, stride=1, padding=1, bias=False),\\\n",
    "                                        nn.AvgPool2d(7))\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def make_layers(self, cfg, batch_norm=True):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for l in cfg:\n",
    "            if l == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, out_channels=l[0], kernel_size=l[1], stride=1, padding=1, bias=False)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(l[0]), nn.LeakyReLU(0.1)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.LeakyReLU(0.1)]\n",
    "                in_channels = l[0]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        print x.shape\n",
    "        x = self.classifier(x)\n",
    "        y = x.view(x.size(0), -1)\n",
    "        return y\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(0.5)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_fm_index(index,conv_cfg):\n",
    "    layer_index = 0\n",
    "    fm_index = 0\n",
    "    while (index >= conv_cfg[layer_index]):\n",
    "        index = index - conv_cfg[layer_index]\n",
    "        layer_index = layer_index + 1\n",
    "    fm_index = index\n",
    "    return layer_index,fm_index\n",
    "\n",
    "def get_index(layer_index,fm_index,conv_cfg):\n",
    "    index = 0\n",
    "    for i in range (layer_index):\n",
    "        index = index + conv_cfg[i]\n",
    "    index = index + fm_index\n",
    "    return index\n",
    "\n",
    "def generate_pruned_cfg(loss_rank, cfg, prune_cnt):\n",
    "\n",
    "    conv_cfg = []# store the fm cnt of each conv layer\n",
    "    pruned_layer_cfg = [[]]# store the remain fm index of each conv layer\n",
    "    pruned_model_cfg = cfg # store pruned cfg to generate new model\n",
    "\n",
    "    for l in cfg:\n",
    "        if l != 'M':\n",
    "            conv_cfg.append(l[0])\n",
    "    \n",
    "    for i in range(len(cfg)-cfg.count('M')-1):\n",
    "        pruned_layer_cfg.append([])\n",
    "    \n",
    "    fm_cnt=len(loss_rank) - prune_cnt#要保留的feature map个数\n",
    "\n",
    "    for i in range(fm_cnt):\n",
    "        layer_index,fm_index=get_layer_fm_index(loss_rank[i],conv_cfg)\n",
    "        pruned_layer_cfg[layer_index].append(fm_index)\n",
    "    \n",
    "    layer_index = 0\n",
    "    for i in range(len(cfg)):\n",
    "        if cfg[i][0] != 'M':\n",
    "            pruned_model_cfg[i][0]=len(pruned_layer_cfg[layer_index])\n",
    "            layer_index+=1\n",
    "    return pruned_model_cfg,pruned_layer_cfg\n",
    "\n",
    "def get_layer_fm_index(index,conv_cfg):\n",
    "    layer_index = 0\n",
    "    fm_index = 0\n",
    "    while (index >= conv_cfg[layer_index]):\n",
    "        index = index - conv_cfg[layer_index]\n",
    "        layer_index = layer_index + 1\n",
    "    fm_index = index\n",
    "    return layer_index,fm_index\n",
    "\n",
    "def get_index(layer_index,fm_index,conv_cfg):\n",
    "    index = 0\n",
    "    for i in range (layer_index):\n",
    "        index = index + conv_cfg[i]\n",
    "    index = index + fm_index\n",
    "    return index\n",
    "\n",
    "def generate_pruned_cfg(loss_rank, cfg, prune_cnt):\n",
    "\n",
    "    conv_cfg = []# store the fm cnt of each conv layer\n",
    "    pruned_layer_cfg = [[]]# store the remain fm index of each conv layer\n",
    "    pruned_model_cfg = cfg # store pruned cfg to generate new model\n",
    "\n",
    "    for l in cfg:\n",
    "        if l != 'M':\n",
    "            conv_cfg.append(l[0])\n",
    "    \n",
    "    for i in range(len(cfg)-cfg.count('M')-1):\n",
    "        pruned_layer_cfg.append([])\n",
    "    \n",
    "    fm_cnt=len(loss_rank) - prune_cnt#要保留的feature map个数\n",
    "\n",
    "    for i in range(fm_cnt):\n",
    "        layer_index,fm_index=get_layer_fm_index(loss_rank[i],conv_cfg)\n",
    "        pruned_layer_cfg[layer_index].append(fm_index)\n",
    "    \n",
    "    layer_index = 0\n",
    "    for i in range(len(cfg)):\n",
    "        if cfg[i][0] != 'M':\n",
    "            pruned_model_cfg[i][0]=len(pruned_layer_cfg[layer_index])\n",
    "            layer_index+=1\n",
    "    return pruned_model_cfg,pruned_layer_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = darknet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.random.random(7200)\n",
    "loss_rank = np.argsort(loss)\n",
    "cfg=[[32,3],'M',[64,3],'M',[128,3],[64,1],[128,3],'M',[256,3],[128,1],[256,3],'M',[512,3],[256,1],[512,3],[256,1],[512,3],'M',[1024,3],[512,1],[1024,3],[512,1],[1024,3]]\n",
    "pruned_model_cfg,pruned_layer_cfg = generate_pruned_cfg(loss_rank, cfg, 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15, 3], 'M', [31, 3], 'M', [68, 3], [41, 1], [67, 3], 'M', [148, 3], [67, 1], [138, 3], 'M', [251, 3], [120, 1], [243, 3], [130, 1], [233, 3], 'M', [523, 3], [258, 1], [520, 3], [243, 1], [504, 3]]\n"
     ]
    }
   ],
   "source": [
    "print pruned_model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darknet(\n",
      "  (feature): Sequential(\n",
      "    (0): Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(15, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.1)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(31, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.1)\n",
      "    (11): Conv2d(68, 41, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.1)\n",
      "    (14): Conv2d(41, 67, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.1)\n",
      "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (18): Conv2d(67, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): BatchNorm2d(148, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): LeakyReLU(negative_slope=0.1)\n",
      "    (21): Conv2d(148, 67, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (22): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): LeakyReLU(negative_slope=0.1)\n",
      "    (24): Conv2d(67, 138, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (25): BatchNorm2d(138, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): LeakyReLU(negative_slope=0.1)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(138, 251, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (29): BatchNorm2d(251, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): LeakyReLU(negative_slope=0.1)\n",
      "    (31): Conv2d(251, 120, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (32): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (33): LeakyReLU(negative_slope=0.1)\n",
      "    (34): Conv2d(120, 243, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (35): BatchNorm2d(243, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): LeakyReLU(negative_slope=0.1)\n",
      "    (37): Conv2d(243, 130, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (38): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): LeakyReLU(negative_slope=0.1)\n",
      "    (40): Conv2d(130, 233, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (41): BatchNorm2d(233, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): LeakyReLU(negative_slope=0.1)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (44): Conv2d(233, 523, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (45): BatchNorm2d(523, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (46): LeakyReLU(negative_slope=0.1)\n",
      "    (47): Conv2d(523, 258, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (48): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (49): LeakyReLU(negative_slope=0.1)\n",
      "    (50): Conv2d(258, 520, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (51): BatchNorm2d(520, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (52): LeakyReLU(negative_slope=0.1)\n",
      "    (53): Conv2d(520, 243, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (54): BatchNorm2d(243, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (55): LeakyReLU(negative_slope=0.1)\n",
      "    (56): Conv2d(243, 504, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (57): BatchNorm2d(504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (58): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Conv2d(504, 30, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pruned_model = darknet(cfg=pruned_model_cfg)\n",
    "print pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6512454b22d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruned_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a1040616f90c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, data_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-974af108a056>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "test(pruned_model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruned_model_init(model, pruned_model, pruned_layer_cfg):\n",
    "    conv_layer_index = 0\n",
    "    for layer_index,pruned_layer in pruned_model.feature._modules.items():\n",
    "        \n",
    "        _,layer = model.feature._modules.items()[int(layer_index)]\n",
    "        \n",
    "        if isinstance(pruned_layer,nn.Conv2d):\n",
    "            \n",
    "            if(conv_layer_index == 0):\n",
    "                weight=layer.weight.data.cpu().numpy()\n",
    "                pruned_weight=layer.weight.data.cpu().numpy()[:len(pruned_layer_cfg[conv_layer_index])]\n",
    "                print pruned_weight.shape\n",
    "                \n",
    "                out_channel=0\n",
    "                for i in pruned_layer_cfg[conv_layer_index]:\n",
    "                    pruned_weight[out_channel]=weight[i]\n",
    "                    out_channel=out_channel+1\n",
    "                \n",
    "                pruned_layer.weight.data = torch.from_numpy(pruned_weight).cuda()\n",
    "                \n",
    "            else:\n",
    "                weight=layer.weight.data.cpu().numpy()\n",
    "                pruned_weight=layer.weight.data.cpu().numpy()[:len(pruned_layer_cfg[conv_layer_index]),:len(pruned_layer_cfg[conv_layer_index - 1])]\n",
    "                print pruned_weight.shape\n",
    "                \n",
    "                in_channel=0\n",
    "                out_channel=0\n",
    "                for i in pruned_layer_cfg[conv_layer_index]:\n",
    "                    for j in pruned_layer_cfg[conv_layer_index - 1]:\n",
    "                        pruned_weight[out_channel,in_channel]=weight[i,j]\n",
    "                        in_channel=in_channel+1\n",
    "                    in_channel=0\n",
    "                    out_channel=out_channel+1\n",
    "                \n",
    "                pruned_layer.weight.data = torch.from_numpy(pruned_weight).cuda()\n",
    "            continue\n",
    "            \n",
    "        elif isinstance(pruned_layer,nn.BatchNorm2d):\n",
    "            \n",
    "            if(conv_layer_index < 18):\n",
    "                \n",
    "                pruned_weight=np.zeros(len(pruned_layer_cfg[conv_layer_index]))\n",
    "                pruned_bias=np.zeros(len(pruned_layer_cfg[conv_layer_index]))\n",
    "                pruned_mean=np.zeros(len(pruned_layer_cfg[conv_layer_index]))\n",
    "                pruned_var=np.zeros(len(pruned_layer_cfg[conv_layer_index]))\n",
    "                \n",
    "                channel=0\n",
    "                for i in pruned_layer_cfg[conv_layer_index]:\n",
    "                    pruned_weight[channel]=layer.weight.data.cpu().numpy()[i]\n",
    "                    pruned_bias[channel]=layer.bias.data.cpu().numpy()[i]\n",
    "                    pruned_mean[channel]=layer.running_mean.data.cpu().numpy()[i]\n",
    "                    pruned_var[channel]=layer.running_var.data.cpu().numpy()[i]\n",
    "                    channel = channel + 1\n",
    "                pruned_layer.weight.data = torch.from_numpy(pruned_weight).cuda()\n",
    "                pruned_layer.bias.data = torch.from_numpy(pruned_bias).cuda()\n",
    "                pruned_layer.running_mean.data = torch.from_numpy(pruned_mean).cuda()\n",
    "                pruned_layer.running_var.data = torch.from_numpy(pruned_var).cuda()\n",
    "\n",
    "            conv_layer_index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3, 3, 3)\n",
      "(31, 15, 3, 3)\n",
      "(68, 31, 3, 3)\n",
      "(41, 68, 1, 1)\n",
      "(67, 41, 3, 3)\n",
      "(148, 67, 3, 3)\n",
      "(67, 148, 1, 1)\n",
      "(138, 67, 3, 3)\n",
      "(251, 138, 3, 3)\n",
      "(120, 251, 1, 1)\n",
      "(243, 120, 3, 3)\n",
      "(130, 243, 1, 1)\n",
      "(233, 130, 3, 3)\n",
      "(523, 233, 3, 3)\n",
      "(258, 523, 1, 1)\n",
      "(520, 258, 3, 3)\n",
      "(243, 520, 1, 1)\n",
      "(504, 243, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('darkenet19.pkl'))\n",
    "pruned_model_init(model, pruned_model, pruned_layer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "train_data_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        #transforms.RandomRotation(10),\n",
    "        #transforms.ColorJitter(hue=0.1,saturation=0.75),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "tset_data_transform = transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='/media/lulugay/PC/CCCV-30/train_set',transform=train_data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=12)\n",
    " \n",
    "val_dataset = torchvision.datasets.ImageFolder(root='/media/lulugay/PC/CCCV-30/test_set', transform=tset_data_transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batchsize, shuffle=True, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = darknet()\n",
    "model.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,data_loader):\n",
    "    net.eval()\n",
    "    corret,total = 0,0\n",
    "    running_loss = 0.0\n",
    "    for inputs,labels in data_loader:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(Variable(inputs))\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        corret += (predicted == labels).sum()\n",
    "        accuracy = 100 * float(corret) / float(total)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        running_loss += loss.data[0]\n",
    "    running_loss = running_loss / (15000 / batchsize)\n",
    "    print('Test Accuracy: %.4f %%, Loss: %.4f' % (accuracy,running_loss))\n",
    "    return accuracy,float(running_loss)\n",
    "\n",
    "def poly(base_lr,power,total_epoch,now_epoch):\n",
    "    return base_lr*(1-math.pow(float(now_epoch)/float(total_epoch),power))\n",
    "\n",
    "def get_layer_fm_index(index):\n",
    "    layer_index = 0\n",
    "    fm_index = 0\n",
    "    while (index >= model_config[layer_index]):\n",
    "        index = index - model_config[layer_index]\n",
    "        layer_index = layer_index + 1\n",
    "    fm_index = index\n",
    "    return layer_index,fm_index\n",
    "\n",
    "def get_index(layer_index,fm_index):\n",
    "    index = 0\n",
    "    for i in range (layer_index):\n",
    "        index = index + model_config[i]\n",
    "    index = index + fm_index\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('epoch49_weight.pkl'))\n",
    "num_epochs = 80\n",
    "for epoch in range(0,num_epochs):\n",
    "    model.train()\n",
    "    batch_size_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    corret,total = 0,0\n",
    "    print('train epoch%d'%epoch)\n",
    "    for i,(inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        inputs = Variable(inputs)\n",
    "        lables = Variable(labels)\n",
    "        lr=poly(0.01,4,num_epochs,epoch)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        corret += (predicted == labels).sum()\n",
    "        loss = loss_func(outputs, labels)        #交叉熵\n",
    "        loss.backward()\n",
    "        optimizer.step()                          #更新权重\n",
    "        running_loss += loss.data[0]\n",
    "    accuracy = 100 * float(corret) / float(total)\n",
    "    running_loss = running_loss / (45000 / batchsize)\n",
    "    print('Train Accuracy: %.4f %%, Loss: %.4f' % (accuracy, running_loss))\n",
    "    print('test epoch%d'%epoch)\n",
    "    test(model,val_loader)\n",
    "    \n",
    "    \n",
    "    print('epoch [%d/%d] need time %.4f' % (epoch + 1, num_epochs, time.time() - batch_size_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5745 2442 4241 ...  525 2430 5853]\n"
     ]
    }
   ],
   "source": [
    "model_config = [32,64,128,64,128,256,128,256,512,256,512,256,512,1024,512,1024,512]\n",
    "loss = np.random.rand(6172)\n",
    "loss_rank = np.argsort(loss)\n",
    "print loss_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
